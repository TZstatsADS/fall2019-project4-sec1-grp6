{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Load Data and Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import random\n",
    "\n",
    "\n",
    "def load_rating_data(file_path=\"../data/ml-latest-small/ratings.csv\"):\n",
    "    \"\"\"\n",
    "    load movie lens 100k ratings from original rating file.\n",
    "    need to download and put rating data in /data folder first.\n",
    "    Source: http://www.grouplens.org/\n",
    "    \"\"\"\n",
    "    prefer = []\n",
    "    for line in open(file_path, 'r'):  # 打开指定文件\n",
    "        (userid, movieid, rating, ts) = line.split(',')  # 数据集中每行有4项\n",
    "        if(userid==\"userId\"): continue\n",
    "        uid = int(userid)\n",
    "        mid = int(movieid)\n",
    "        rat = float(rating)\n",
    "        prefer.append([uid, mid, rat])\n",
    "    data = array(prefer)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000e+00, 1.00000e+00, 4.00000e+00],\n",
       "       [1.00000e+00, 3.00000e+00, 4.00000e+00],\n",
       "       [1.00000e+00, 6.00000e+00, 4.00000e+00],\n",
       "       ...,\n",
       "       [6.10000e+02, 1.68250e+05, 5.00000e+00],\n",
       "       [6.10000e+02, 1.68252e+05, 5.00000e+00],\n",
       "       [6.10000e+02, 1.70875e+05, 3.00000e+00]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=load_rating_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data[data[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(new_data[:, 1])\n",
    "a=le.transform(new_data[:, 1])\n",
    "c=np.array(a).reshape(100836,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = np.concatenate((new_data,c),axis=1)\n",
    "final_data=final_data[:,[0,3,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Matrix Factorization (probabilistic-matrix-factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PMF(object):\n",
    "    def __init__(self, num_feat=10, epsilon=1, _lambda1=0.1,_lambda2=0.1, momentum=0.8, maxepoch=20, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat  # Number of latent features,\n",
    "        self.epsilon = epsilon  # learning rate,\n",
    "        self._lambda1 = _lambda1  # L2 regularization,\n",
    "        self._lambda2 = _lambda2  # L2 regularization,\n",
    "        self.momentum = momentum  # momentum of the gradient,\n",
    "        self.maxepoch = maxepoch  # Number of epoch before stop,\n",
    "        self.num_batches = num_batches  # Number of batches in each epoch (for SGD optimization),\n",
    "        self.batch_size = batch_size  # Number of training samples used in each batches (for SGD optimization)\n",
    "\n",
    "        self.w_Item = None  # Item feature vectors\n",
    "        self.w_User = None  # User feature vectors\n",
    "\n",
    "        self.rmse_train = []\n",
    "        self.rmse_test = []\n",
    "\n",
    "    # ***Fit the model with train_tuple and evaluate RMSE on both train and test data.  ***********#\n",
    "    # ***************** train_vec=TrainData, test_vec=TestData*************#\n",
    "    \n",
    "    def fit(self, train_vec, test_vec):\n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:, 2])  # rating mean\n",
    "\n",
    "        pairs_train = train_vec.shape[0]  # the number of lines in traindata \n",
    "        pairs_test = test_vec.shape[0]  # the number of lines in testdata\n",
    "\n",
    "        # 1-p-i, 2-m-c\n",
    "        num_user = int(max(np.amax(train_vec[:, 0]), np.amax(test_vec[:, 0]))) + 1  # the total number of user\n",
    "        num_item = int(max(np.amax(train_vec[:, 1]), np.amax(test_vec[:, 1]))) + 1  # the total number of movie\n",
    "        print(num_item)\n",
    "\n",
    "        incremental = False  \n",
    "        if ((not incremental) or (self.w_Item is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_Item = 0.1 * np.random.randn(num_item, self.num_feat)  # numpy.random.randn movie: M x D normal-distributed-matrix\n",
    "            self.w_User = 0.1 * np.random.randn(num_user, self.num_feat)  # numpy.random.randn user: N x D normal-distributed-matrix\n",
    "\n",
    "            self.w_Item_inc = np.zeros((num_item, self.num_feat))  # movie: M x D 0-matrix\n",
    "            self.w_User_inc = np.zeros((num_user, self.num_feat))  # user: N x D 0-matrix\n",
    "\n",
    "        while self.epoch < self.maxepoch:  # check the number of iterations\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])  \n",
    "            np.random.shuffle(shuffled_order)  \n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):  \n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                test = np.arange(self.batch_size * batch, self.batch_size * (batch + 1))\n",
    "                batch_idx = np.mod(test, shuffled_order.shape[0])  \n",
    "\n",
    "                batch_UserID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_ItemID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_User[batch_UserID, :],\n",
    "                                              self.w_Item[batch_ItemID, :]),\n",
    "                                  axis=1)  \n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_User = 2 * np.multiply(rawErr[:, np.newaxis], self.w_Item[batch_ItemID, :]) \\\n",
    "                       + self._lambda1 * self.w_User[batch_UserID, :]\n",
    "                Ix_Item = 2 * np.multiply(rawErr[:, np.newaxis], self.w_User[batch_UserID, :]) \\\n",
    "                       + self._lambda2 * (self.w_Item[batch_ItemID, :])  # np.newaxis :increase the dimension\n",
    "\n",
    "                dw_Item = np.zeros((num_item, self.num_feat))\n",
    "                dw_User = np.zeros((num_user, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_Item[batch_ItemID[i], :] += Ix_Item[i, :]\n",
    "                    dw_User[batch_UserID[i], :] += Ix_User[i, :]\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_Item_inc = self.momentum * self.w_Item_inc + self.epsilon * dw_Item / self.batch_size\n",
    "                self.w_User_inc = self.momentum * self.w_User_inc + self.epsilon * dw_User / self.batch_size\n",
    "\n",
    "                self.w_Item = self.w_Item - self.w_Item_inc\n",
    "                self.w_User = self.w_User - self.w_User_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_User[np.array(train_vec[:, 0], dtype='int32'), :],\n",
    "                                                  self.w_Item[np.array(train_vec[:, 1], dtype='int32'), :]),\n",
    "                                      axis=1)  # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = 0.5*np.linalg.norm(rawErr) ** 2 \\\n",
    "                          + 0.5 * self._lambda1 * np.linalg.norm(self.w_User) ** 2 + 0.5 * self._lambda2 * np.linalg.norm(self.w_Item) ** 2\n",
    "\n",
    "                    self.rmse_train.append(np.sqrt(obj / pairs_train))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_User[np.array(test_vec[:, 0], dtype='int32'), :],\n",
    "                                                  self.w_Item[np.array(test_vec[:, 1], dtype='int32'), :]),\n",
    "                                      axis=1)  # mean_inv subtracted\n",
    "                    rawErr = pred_out - test_vec[:, 2] + self.mean_inv\n",
    "                    self.rmse_test.append(np.linalg.norm(rawErr) / np.sqrt(pairs_test))\n",
    "\n",
    "                    # Print info\n",
    "                    if batch == self.num_batches - 1:\n",
    "                        print('Training RMSE: %f, Test RMSE %f' % (self.rmse_train[-1], self.rmse_test[-1]))\n",
    "\n",
    "    def predict(self):\n",
    "        return np.dot(self.w_Item, self.w_User) + self.mean_inv  \n",
    "\n",
    "    # ****************Set parameters by providing a parameter dictionary.  ***********#\n",
    "    def set_params(self, parameters):\n",
    "        if isinstance(parameters, dict):\n",
    "            self.num_feat = parameters.get(\"num_feat\", 10)\n",
    "            self.epsilon = parameters.get(\"epsilon\", 1)\n",
    "            self._lambda1 = parameters.get(\"_lambda1\", 0.1)\n",
    "            self._lambda2 = parameters.get(\"_lambda2\", 0.1)\n",
    "            self.momentum = parameters.get(\"momentum\", 0.8)\n",
    "            self.maxepoch = parameters.get(\"maxepoch\", 20)\n",
    "            self.num_batches = parameters.get(\"num_batches\", 10)\n",
    "            self.batch_size = parameters.get(\"batch_size\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 9724 10\n",
      "9724\n",
      "Training RMSE: 0.737399, Test RMSE 1.039101\n",
      "Training RMSE: 0.736145, Test RMSE 1.039001\n",
      "Training RMSE: 0.734607, Test RMSE 1.038610\n",
      "Training RMSE: 0.732120, Test RMSE 1.037399\n",
      "Training RMSE: 0.727254, Test RMSE 1.033940\n",
      "Training RMSE: 0.717163, Test RMSE 1.025236\n",
      "Training RMSE: 0.699006, Test RMSE 1.008388\n",
      "Training RMSE: 0.674517, Test RMSE 0.986100\n",
      "Training RMSE: 0.648694, Test RMSE 0.964489\n",
      "Training RMSE: 0.624617, Test RMSE 0.946530\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #file_path = \"data/ml-100k/ratings.csv\"\n",
    "    pmf = PMF()\n",
    "    pmf.set_params({\"num_feat\": 10, \"epsilon\": 1, \"_lambda\": 0.1, \"momentum\": 0.8, \"maxepoch\": 10, \"num_batches\": 100,\n",
    "                    \"batch_size\": 1000})\n",
    "    print(len(np.unique(final_data[:, 0])), len(np.unique(final_data[:, 1])), pmf.num_feat)\n",
    "    train, test = train_test_split(final_data)\n",
    "    pmf.fit(train, test)\n",
    "\n",
    "    # Check performance by plotting train and test errors\n",
    "    plt.plot(range(pmf.maxepoch), pmf.rmse_train, marker='o', label='Training Data')\n",
    "    plt.plot(range(pmf.maxepoch), pmf.rmse_test, marker='v', label='Test Data')\n",
    "    plt.title('The MovieRating Dataset Learning Curve')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pred_rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.dot(pmf.w_User,pmf.w_Item.T) + pmf.mean_inv \n",
    "pred_matrix_final=a[:-1,]\n",
    "np.shape(pred_matrix_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.2 Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat_list=np.arange(10,30,10)\n",
    "lambda1_list=np.array([0.01,0.1])\n",
    "lambda2_list=np.array([0.01,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros((8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-191-4835a8307e49>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-191-4835a8307e49>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    a[,2]=0.1\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a[0:4,0]=10\n",
    "a[c(0,1,4,5),1]=0.01\n",
    "a[,2]=0.1\n",
    "a[4:,0]=20\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z=meshgrid(num_feat_list,lambda1_list,lambda2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_latent = 10 lambda for user = 0.01 lambda for movie = 0.01\n",
      "9724\n",
      "Training RMSE: 0.735262, Test RMSE 1.050196\n",
      "Training RMSE: 0.734031, Test RMSE 1.050081\n",
      "Training RMSE: 0.732353, Test RMSE 1.049989\n",
      "Training RMSE: 0.729169, Test RMSE 1.049905\n",
      "Training RMSE: 0.722163, Test RMSE 1.049747\n",
      "Training RMSE: 0.707304, Test RMSE 1.049630\n",
      "Training RMSE: 0.683121, Test RMSE 1.049644\n",
      "Training RMSE: 0.655183, Test RMSE 1.049837\n",
      "Training RMSE: 0.630495, Test RMSE 1.050077\n",
      "Training RMSE: 0.609955, Test RMSE 1.050253\n",
      "9724\n",
      "Training RMSE: 0.735031, Test RMSE 1.054235\n",
      "Training RMSE: 0.733862, Test RMSE 1.054254\n",
      "Training RMSE: 0.732258, Test RMSE 1.054343\n",
      "Training RMSE: 0.729076, Test RMSE 1.054580\n",
      "Training RMSE: 0.721854, Test RMSE 1.055068\n",
      "Training RMSE: 0.707303, Test RMSE 1.055924\n",
      "Training RMSE: 0.686374, Test RMSE 1.057107\n",
      "Training RMSE: 0.663237, Test RMSE 1.058382\n",
      "Training RMSE: 0.640156, Test RMSE 1.059595\n",
      "Training RMSE: 0.618926, Test RMSE 1.060720\n",
      "9724\n",
      "Training RMSE: 0.731858, Test RMSE 1.069036\n",
      "Training RMSE: 0.730397, Test RMSE 1.068972\n",
      "Training RMSE: 0.727995, Test RMSE 1.069023\n",
      "Training RMSE: 0.722569, Test RMSE 1.069285\n",
      "Training RMSE: 0.710311, Test RMSE 1.069769\n",
      "Training RMSE: 0.689683, Test RMSE 1.070745\n",
      "Training RMSE: 0.665992, Test RMSE 1.071832\n",
      "Training RMSE: 0.642548, Test RMSE 1.072927\n",
      "Training RMSE: 0.620707, Test RMSE 1.073816\n",
      "Training RMSE: 0.601281, Test RMSE 1.074533\n",
      "9724\n",
      "Training RMSE: 0.740055, Test RMSE 1.022902\n",
      "Training RMSE: 0.738852, Test RMSE 1.022876\n",
      "Training RMSE: 0.737265, Test RMSE 1.022907\n",
      "Training RMSE: 0.734261, Test RMSE 1.022977\n",
      "Training RMSE: 0.727280, Test RMSE 1.023233\n",
      "Training RMSE: 0.711714, Test RMSE 1.023808\n",
      "Training RMSE: 0.687128, Test RMSE 1.024926\n",
      "Training RMSE: 0.661809, Test RMSE 1.026042\n",
      "Training RMSE: 0.639394, Test RMSE 1.026966\n",
      "Training RMSE: 0.618934, Test RMSE 1.027806\n",
      "9724\n",
      "Training RMSE: 0.739599, Test RMSE 1.025328\n",
      "Training RMSE: 0.738373, Test RMSE 1.025265\n",
      "Training RMSE: 0.736607, Test RMSE 1.025311\n",
      "Training RMSE: 0.732817, Test RMSE 1.025552\n",
      "Training RMSE: 0.722999, Test RMSE 1.026137\n",
      "Training RMSE: 0.701187, Test RMSE 1.027387\n",
      "Training RMSE: 0.670945, Test RMSE 1.028920\n",
      "Training RMSE: 0.643040, Test RMSE 1.030078\n",
      "Training RMSE: 0.620420, Test RMSE 1.030936\n",
      "Training RMSE: 0.602537, Test RMSE 1.031510\n",
      "[0.73959904 0.73837285 0.73660715 0.73281713 0.72299863 0.70118689\n",
      " 0.67094545 0.64304008 0.6204196  0.60253656]\n",
      "[1.02532776 1.02526462 1.02531058 1.02555224 1.02613749 1.02738713\n",
      " 1.0289198  1.03007783 1.03093603 1.03150975]\n",
      "num_latent = 10 lambda for user = 0.01 lambda for movie = 0.05\n",
      "9724\n",
      "Training RMSE: 0.735105, Test RMSE 1.050424\n",
      "Training RMSE: 0.733831, Test RMSE 1.050341\n",
      "Training RMSE: 0.731941, Test RMSE 1.050258\n",
      "Training RMSE: 0.728105, Test RMSE 1.050148\n",
      "Training RMSE: 0.719634, Test RMSE 1.050075\n",
      "Training RMSE: 0.702955, Test RMSE 1.050091\n",
      "Training RMSE: 0.679805, Test RMSE 1.050365\n",
      "Training RMSE: 0.654678, Test RMSE 1.050917\n",
      "Training RMSE: 0.631267, Test RMSE 1.051491\n",
      "Training RMSE: 0.611044, Test RMSE 1.051979\n",
      "9724\n",
      "Training RMSE: 0.734869, Test RMSE 1.053819\n",
      "Training RMSE: 0.733349, Test RMSE 1.053847\n",
      "Training RMSE: 0.730748, Test RMSE 1.054010\n",
      "Training RMSE: 0.725025, Test RMSE 1.054370\n",
      "Training RMSE: 0.712535, Test RMSE 1.055060\n",
      "Training RMSE: 0.692618, Test RMSE 1.056104\n",
      "Training RMSE: 0.670358, Test RMSE 1.057193\n",
      "Training RMSE: 0.648308, Test RMSE 1.058108\n",
      "Training RMSE: 0.627317, Test RMSE 1.058976\n",
      "Training RMSE: 0.608567, Test RMSE 1.059666\n",
      "9724\n",
      "Training RMSE: 0.731806, Test RMSE 1.068755\n",
      "Training RMSE: 0.730416, Test RMSE 1.068718\n",
      "Training RMSE: 0.728167, Test RMSE 1.068753\n",
      "Training RMSE: 0.723256, Test RMSE 1.068872\n",
      "Training RMSE: 0.711885, Test RMSE 1.069306\n",
      "Training RMSE: 0.691067, Test RMSE 1.070074\n",
      "Training RMSE: 0.665563, Test RMSE 1.071108\n",
      "Training RMSE: 0.641245, Test RMSE 1.072044\n",
      "Training RMSE: 0.619220, Test RMSE 1.072896\n",
      "Training RMSE: 0.600943, Test RMSE 1.073508\n",
      "9724\n",
      "Training RMSE: 0.740074, Test RMSE 1.023415\n",
      "Training RMSE: 0.738838, Test RMSE 1.023321\n",
      "Training RMSE: 0.737206, Test RMSE 1.023337\n",
      "Training RMSE: 0.734107, Test RMSE 1.023501\n",
      "Training RMSE: 0.726769, Test RMSE 1.023875\n",
      "Training RMSE: 0.710152, Test RMSE 1.024699\n",
      "Training RMSE: 0.683269, Test RMSE 1.026068\n",
      "Training RMSE: 0.655273, Test RMSE 1.027443\n",
      "Training RMSE: 0.631477, Test RMSE 1.028475\n",
      "Training RMSE: 0.611317, Test RMSE 1.029297\n",
      "9724\n",
      "Training RMSE: 0.739475, Test RMSE 1.025324\n",
      "Training RMSE: 0.737774, Test RMSE 1.025307\n",
      "Training RMSE: 0.734327, Test RMSE 1.025530\n",
      "Training RMSE: 0.725238, Test RMSE 1.026063\n",
      "Training RMSE: 0.704949, Test RMSE 1.027218\n",
      "Training RMSE: 0.677965, Test RMSE 1.028680\n",
      "Training RMSE: 0.653505, Test RMSE 1.029933\n",
      "Training RMSE: 0.632113, Test RMSE 1.030986\n",
      "Training RMSE: 0.613022, Test RMSE 1.031997\n",
      "Training RMSE: 0.597307, Test RMSE 1.032747\n",
      "[0.73947547 0.73777437 0.73432691 0.72523791 0.70494934 0.67796534\n",
      " 0.65350533 0.63211317 0.61302193 0.59730719]\n",
      "[1.02532371 1.02530729 1.02553025 1.02606343 1.02721773 1.02867983\n",
      " 1.0299327  1.03098576 1.03199735 1.03274653]\n",
      "num_latent = 10 lambda for user = 0.01 lambda for movie = 0.1\n",
      "9724\n",
      "Training RMSE: 0.735253, Test RMSE 1.050248\n",
      "Training RMSE: 0.734067, Test RMSE 1.050194\n",
      "Training RMSE: 0.732400, Test RMSE 1.050200\n",
      "Training RMSE: 0.729140, Test RMSE 1.050278\n",
      "Training RMSE: 0.721850, Test RMSE 1.050488\n",
      "Training RMSE: 0.706500, Test RMSE 1.050917\n",
      "Training RMSE: 0.683186, Test RMSE 1.051463\n",
      "Training RMSE: 0.657930, Test RMSE 1.052082\n",
      "Training RMSE: 0.634652, Test RMSE 1.052545\n",
      "Training RMSE: 0.614734, Test RMSE 1.052955\n",
      "9724\n",
      "Training RMSE: 0.734916, Test RMSE 1.053829\n",
      "Training RMSE: 0.733471, Test RMSE 1.053808\n",
      "Training RMSE: 0.731182, Test RMSE 1.053811\n",
      "Training RMSE: 0.726245, Test RMSE 1.053891\n",
      "Training RMSE: 0.715331, Test RMSE 1.054132\n",
      "Training RMSE: 0.695885, Test RMSE 1.054676\n",
      "Training RMSE: 0.671672, Test RMSE 1.055442\n",
      "Training RMSE: 0.647172, Test RMSE 1.056333\n",
      "Training RMSE: 0.624699, Test RMSE 1.057149\n",
      "Training RMSE: 0.606218, Test RMSE 1.057715\n",
      "9724\n",
      "Training RMSE: 0.731890, Test RMSE 1.068833\n",
      "Training RMSE: 0.730523, Test RMSE 1.068873\n",
      "Training RMSE: 0.728477, Test RMSE 1.068914\n",
      "Training RMSE: 0.724246, Test RMSE 1.069062\n",
      "Training RMSE: 0.714376, Test RMSE 1.069379\n",
      "Training RMSE: 0.694751, Test RMSE 1.070008\n",
      "Training RMSE: 0.667633, Test RMSE 1.070959\n",
      "Training RMSE: 0.640888, Test RMSE 1.071962\n",
      "Training RMSE: 0.617425, Test RMSE 1.072811\n",
      "Training RMSE: 0.598335, Test RMSE 1.073375\n",
      "9724\n",
      "Training RMSE: 0.739983, Test RMSE 1.023154\n",
      "Training RMSE: 0.738606, Test RMSE 1.023282\n",
      "Training RMSE: 0.736569, Test RMSE 1.023518\n",
      "Training RMSE: 0.732085, Test RMSE 1.023929\n",
      "Training RMSE: 0.721120, Test RMSE 1.024802\n",
      "Training RMSE: 0.699926, Test RMSE 1.026254\n",
      "Training RMSE: 0.674298, Test RMSE 1.027775\n",
      "Training RMSE: 0.650796, Test RMSE 1.029064\n",
      "Training RMSE: 0.629267, Test RMSE 1.030159\n",
      "Training RMSE: 0.610136, Test RMSE 1.031053\n",
      "9724\n",
      "Training RMSE: 0.739555, Test RMSE 1.025607\n",
      "Training RMSE: 0.738002, Test RMSE 1.025589\n",
      "Training RMSE: 0.735052, Test RMSE 1.025723\n",
      "Training RMSE: 0.727365, Test RMSE 1.026035\n",
      "Training RMSE: 0.708956, Test RMSE 1.026790\n",
      "Training RMSE: 0.681173, Test RMSE 1.027983\n",
      "Training RMSE: 0.654379, Test RMSE 1.029072\n",
      "Training RMSE: 0.631272, Test RMSE 1.029934\n",
      "Training RMSE: 0.612150, Test RMSE 1.030533\n",
      "Training RMSE: 0.596066, Test RMSE 1.031131\n",
      "[0.7395547  0.73800178 0.73505198 0.72736531 0.70895618 0.68117308\n",
      " 0.65437851 0.63127188 0.61214955 0.59606609]\n",
      "[1.02560749 1.02558922 1.02572342 1.02603472 1.02679049 1.02798311\n",
      " 1.02907186 1.02993436 1.03053314 1.03113063]\n",
      "num_latent = 10 lambda for user = 0.05 lambda for movie = 0.01\n",
      "9724\n",
      "Training RMSE: 0.735139, Test RMSE 1.050224\n",
      "Training RMSE: 0.733807, Test RMSE 1.050182\n",
      "Training RMSE: 0.731770, Test RMSE 1.050186\n",
      "Training RMSE: 0.727450, Test RMSE 1.050279\n",
      "Training RMSE: 0.717857, Test RMSE 1.050453\n",
      "Training RMSE: 0.700481, Test RMSE 1.050880\n",
      "Training RMSE: 0.678363, Test RMSE 1.051493\n",
      "Training RMSE: 0.655260, Test RMSE 1.052184\n",
      "Training RMSE: 0.632733, Test RMSE 1.052883\n",
      "Training RMSE: 0.612434, Test RMSE 1.053480\n",
      "9724\n",
      "Training RMSE: 0.734982, Test RMSE 1.054049\n",
      "Training RMSE: 0.733545, Test RMSE 1.054045\n",
      "Training RMSE: 0.731236, Test RMSE 1.054102\n",
      "Training RMSE: 0.726209, Test RMSE 1.054299\n",
      "Training RMSE: 0.715028, Test RMSE 1.054712\n",
      "Training RMSE: 0.695349, Test RMSE 1.055551\n",
      "Training RMSE: 0.671759, Test RMSE 1.056676\n",
      "Training RMSE: 0.648138, Test RMSE 1.057864\n",
      "Training RMSE: 0.626311, Test RMSE 1.058912\n",
      "Training RMSE: 0.607229, Test RMSE 1.059771\n",
      "9724\n",
      "Training RMSE: 0.731995, Test RMSE 1.068729\n",
      "Training RMSE: 0.730563, Test RMSE 1.068661\n",
      "Training RMSE: 0.728163, Test RMSE 1.068726\n",
      "Training RMSE: 0.722627, Test RMSE 1.069011\n",
      "Training RMSE: 0.710000, Test RMSE 1.069625\n",
      "Training RMSE: 0.689872, Test RMSE 1.070691\n",
      "Training RMSE: 0.668476, Test RMSE 1.071885\n",
      "Training RMSE: 0.647565, Test RMSE 1.073045\n",
      "Training RMSE: 0.627052, Test RMSE 1.074023\n",
      "Training RMSE: 0.607895, Test RMSE 1.074887\n",
      "9724\n",
      "Training RMSE: 0.740033, Test RMSE 1.023171\n",
      "Training RMSE: 0.738656, Test RMSE 1.023161\n",
      "Training RMSE: 0.736512, Test RMSE 1.023204\n",
      "Training RMSE: 0.731651, Test RMSE 1.023336\n",
      "Training RMSE: 0.719929, Test RMSE 1.023763\n",
      "Training RMSE: 0.698093, Test RMSE 1.024603\n",
      "Training RMSE: 0.671801, Test RMSE 1.025618\n",
      "Training RMSE: 0.646431, Test RMSE 1.026682\n",
      "Training RMSE: 0.623824, Test RMSE 1.027507\n",
      "Training RMSE: 0.604879, Test RMSE 1.028128\n",
      "9724\n",
      "Training RMSE: 0.739580, Test RMSE 1.025115\n",
      "Training RMSE: 0.738299, Test RMSE 1.025065\n",
      "Training RMSE: 0.736345, Test RMSE 1.025103\n",
      "Training RMSE: 0.731776, Test RMSE 1.025148\n",
      "Training RMSE: 0.720136, Test RMSE 1.025393\n",
      "Training RMSE: 0.698094, Test RMSE 1.026004\n",
      "Training RMSE: 0.672233, Test RMSE 1.026892\n",
      "Training RMSE: 0.648183, Test RMSE 1.027787\n",
      "Training RMSE: 0.626425, Test RMSE 1.028765\n",
      "Training RMSE: 0.607678, Test RMSE 1.029661\n",
      "[0.73958033 0.73829855 0.73634484 0.73177631 0.72013648 0.69809392\n",
      " 0.67223297 0.64818292 0.62642524 0.60767798]\n",
      "[1.02511481 1.02506547 1.02510279 1.02514848 1.02539254 1.02600356\n",
      " 1.02689211 1.02778746 1.02876499 1.02966069]\n",
      "num_latent = 10 lambda for user = 0.05 lambda for movie = 0.05\n",
      "9724\n",
      "Training RMSE: 0.735048, Test RMSE 1.050122\n",
      "Training RMSE: 0.733735, Test RMSE 1.050109\n",
      "Training RMSE: 0.731851, Test RMSE 1.050200\n",
      "Training RMSE: 0.728340, Test RMSE 1.050348\n",
      "Training RMSE: 0.720657, Test RMSE 1.050711\n",
      "Training RMSE: 0.704566, Test RMSE 1.051375\n",
      "Training RMSE: 0.678897, Test RMSE 1.052379\n",
      "Training RMSE: 0.650437, Test RMSE 1.053469\n",
      "Training RMSE: 0.626453, Test RMSE 1.054214\n",
      "Training RMSE: 0.607013, Test RMSE 1.054630\n",
      "9724\n",
      "Training RMSE: 0.735080, Test RMSE 1.053710\n",
      "Training RMSE: 0.733786, Test RMSE 1.053668\n",
      "Training RMSE: 0.731930, Test RMSE 1.053723\n",
      "Training RMSE: 0.728174, Test RMSE 1.053859\n",
      "Training RMSE: 0.719515, Test RMSE 1.054232\n",
      "Training RMSE: 0.701730, Test RMSE 1.054997\n",
      "Training RMSE: 0.676162, Test RMSE 1.056151\n",
      "Training RMSE: 0.650115, Test RMSE 1.057331\n",
      "Training RMSE: 0.626942, Test RMSE 1.058300\n",
      "Training RMSE: 0.607737, Test RMSE 1.058931\n",
      "9724\n",
      "Training RMSE: 0.731866, Test RMSE 1.069094\n",
      "Training RMSE: 0.730484, Test RMSE 1.069108\n",
      "Training RMSE: 0.728372, Test RMSE 1.069226\n",
      "Training RMSE: 0.723693, Test RMSE 1.069507\n",
      "Training RMSE: 0.712811, Test RMSE 1.070142\n",
      "Training RMSE: 0.693196, Test RMSE 1.071264\n",
      "Training RMSE: 0.669681, Test RMSE 1.072664\n",
      "Training RMSE: 0.646467, Test RMSE 1.074142\n",
      "Training RMSE: 0.623480, Test RMSE 1.075534\n",
      "Training RMSE: 0.603389, Test RMSE 1.076572\n",
      "9724\n",
      "Training RMSE: 0.740011, Test RMSE 1.023168\n",
      "Training RMSE: 0.738635, Test RMSE 1.023067\n",
      "Training RMSE: 0.736571, Test RMSE 1.023020\n",
      "Training RMSE: 0.732114, Test RMSE 1.023120\n",
      "Training RMSE: 0.721425, Test RMSE 1.023426\n",
      "Training RMSE: 0.700694, Test RMSE 1.024226\n",
      "Training RMSE: 0.675011, Test RMSE 1.025270\n",
      "Training RMSE: 0.651432, Test RMSE 1.026295\n",
      "Training RMSE: 0.629207, Test RMSE 1.027314\n",
      "Training RMSE: 0.609983, Test RMSE 1.028205\n",
      "9724\n",
      "Training RMSE: 0.739473, Test RMSE 1.025607\n",
      "Training RMSE: 0.737973, Test RMSE 1.025556\n",
      "Training RMSE: 0.735298, Test RMSE 1.025635\n",
      "Training RMSE: 0.728536, Test RMSE 1.025843\n",
      "Training RMSE: 0.711555, Test RMSE 1.026424\n",
      "Training RMSE: 0.683727, Test RMSE 1.027390\n",
      "Training RMSE: 0.655819, Test RMSE 1.028270\n",
      "Training RMSE: 0.631835, Test RMSE 1.029047\n",
      "Training RMSE: 0.611762, Test RMSE 1.029638\n",
      "Training RMSE: 0.595352, Test RMSE 1.030156\n",
      "[0.73947311 0.73797304 0.73529798 0.72853628 0.71155547 0.68372678\n",
      " 0.65581916 0.63183515 0.61176207 0.59535163]\n",
      "[1.02560706 1.0255558  1.02563528 1.02584253 1.02642431 1.02739006\n",
      " 1.02826965 1.0290474  1.02963751 1.03015563]\n",
      "num_latent = 10 lambda for user = 0.05 lambda for movie = 0.1\n",
      "9724\n",
      "Training RMSE: 0.735229, Test RMSE 1.050189\n",
      "Training RMSE: 0.734065, Test RMSE 1.050249\n",
      "Training RMSE: 0.732528, Test RMSE 1.050379\n",
      "Training RMSE: 0.729745, Test RMSE 1.050606\n",
      "Training RMSE: 0.723780, Test RMSE 1.051078\n",
      "Training RMSE: 0.710808, Test RMSE 1.051954\n",
      "Training RMSE: 0.688928, Test RMSE 1.053224\n",
      "Training RMSE: 0.662925, Test RMSE 1.054520\n",
      "Training RMSE: 0.638039, Test RMSE 1.055556\n",
      "Training RMSE: 0.615938, Test RMSE 1.056295\n",
      "9724\n",
      "Training RMSE: 0.735053, Test RMSE 1.053936\n",
      "Training RMSE: 0.733827, Test RMSE 1.053893\n",
      "Training RMSE: 0.732152, Test RMSE 1.053893\n",
      "Training RMSE: 0.728804, Test RMSE 1.053970\n",
      "Training RMSE: 0.720946, Test RMSE 1.054145\n",
      "Training RMSE: 0.705057, Test RMSE 1.054571\n",
      "Training RMSE: 0.682924, Test RMSE 1.055192\n",
      "Training RMSE: 0.659926, Test RMSE 1.055843\n",
      "Training RMSE: 0.638257, Test RMSE 1.056324\n",
      "Training RMSE: 0.617684, Test RMSE 1.056804\n",
      "9724\n",
      "Training RMSE: 0.731884, Test RMSE 1.069151\n",
      "Training RMSE: 0.730414, Test RMSE 1.069196\n",
      "Training RMSE: 0.727852, Test RMSE 1.069433\n",
      "Training RMSE: 0.721880, Test RMSE 1.069927\n",
      "Training RMSE: 0.708536, Test RMSE 1.070966\n",
      "Training RMSE: 0.687603, Test RMSE 1.072454\n",
      "Training RMSE: 0.664847, Test RMSE 1.074035\n",
      "Training RMSE: 0.642211, Test RMSE 1.075504\n",
      "Training RMSE: 0.621035, Test RMSE 1.076678\n",
      "Training RMSE: 0.602600, Test RMSE 1.077522\n",
      "9724\n",
      "Training RMSE: 0.739920, Test RMSE 1.023073\n",
      "Training RMSE: 0.738417, Test RMSE 1.023029\n",
      "Training RMSE: 0.736042, Test RMSE 1.023064\n",
      "Training RMSE: 0.730771, Test RMSE 1.023184\n",
      "Training RMSE: 0.718477, Test RMSE 1.023587\n",
      "Training RMSE: 0.696337, Test RMSE 1.024435\n",
      "Training RMSE: 0.670100, Test RMSE 1.025579\n",
      "Training RMSE: 0.644767, Test RMSE 1.026767\n",
      "Training RMSE: 0.622729, Test RMSE 1.027840\n",
      "Training RMSE: 0.604028, Test RMSE 1.028586\n",
      "9724\n",
      "Training RMSE: 0.739556, Test RMSE 1.025093\n",
      "Training RMSE: 0.738060, Test RMSE 1.025104\n",
      "Training RMSE: 0.735312, Test RMSE 1.025216\n",
      "Training RMSE: 0.728297, Test RMSE 1.025540\n",
      "Training RMSE: 0.711004, Test RMSE 1.026391\n",
      "Training RMSE: 0.682517, Test RMSE 1.027733\n",
      "Training RMSE: 0.653870, Test RMSE 1.028883\n",
      "Training RMSE: 0.630239, Test RMSE 1.029629\n",
      "Training RMSE: 0.611533, Test RMSE 1.030045\n",
      "Training RMSE: 0.596395, Test RMSE 1.030376\n",
      "[0.73955627 0.73806046 0.73531246 0.72829727 0.71100423 0.68251677\n",
      " 0.65387017 0.63023858 0.61153254 0.59639512]\n",
      "[1.02509254 1.0251042  1.02521631 1.02553983 1.02639145 1.02773294\n",
      " 1.02888292 1.0296288  1.03004503 1.03037582]\n",
      "num_latent = 10 lambda for user = 0.1 lambda for movie = 0.01\n",
      "9724\n",
      "Training RMSE: 0.735059, Test RMSE 1.050178\n",
      "Training RMSE: 0.733740, Test RMSE 1.050120\n",
      "Training RMSE: 0.731794, Test RMSE 1.050061\n",
      "Training RMSE: 0.727813, Test RMSE 1.049972\n",
      "Training RMSE: 0.718949, Test RMSE 1.049965\n",
      "Training RMSE: 0.701560, Test RMSE 1.050125\n",
      "Training RMSE: 0.677463, Test RMSE 1.050510\n",
      "Training RMSE: 0.653068, Test RMSE 1.050947\n",
      "Training RMSE: 0.630111, Test RMSE 1.051440\n",
      "Training RMSE: 0.609822, Test RMSE 1.051935\n",
      "9724\n",
      "Training RMSE: 0.735017, Test RMSE 1.053623\n",
      "Training RMSE: 0.733730, Test RMSE 1.053552\n",
      "Training RMSE: 0.731899, Test RMSE 1.053583\n",
      "Training RMSE: 0.728239, Test RMSE 1.053615\n",
      "Training RMSE: 0.719989, Test RMSE 1.053802\n",
      "Training RMSE: 0.703445, Test RMSE 1.054266\n",
      "Training RMSE: 0.680235, Test RMSE 1.055052\n",
      "Training RMSE: 0.656799, Test RMSE 1.055829\n",
      "Training RMSE: 0.634128, Test RMSE 1.056705\n",
      "Training RMSE: 0.613956, Test RMSE 1.057494\n",
      "9724\n",
      "Training RMSE: 0.732019, Test RMSE 1.068936\n",
      "Training RMSE: 0.730717, Test RMSE 1.068981\n",
      "Training RMSE: 0.728835, Test RMSE 1.069177\n",
      "Training RMSE: 0.724982, Test RMSE 1.069567\n",
      "Training RMSE: 0.715888, Test RMSE 1.070326\n",
      "Training RMSE: 0.697671, Test RMSE 1.071649\n",
      "Training RMSE: 0.673319, Test RMSE 1.073096\n",
      "Training RMSE: 0.648567, Test RMSE 1.074389\n",
      "Training RMSE: 0.625638, Test RMSE 1.075329\n",
      "Training RMSE: 0.604992, Test RMSE 1.076050\n",
      "9724\n",
      "Training RMSE: 0.739946, Test RMSE 1.022938\n",
      "Training RMSE: 0.738652, Test RMSE 1.022949\n",
      "Training RMSE: 0.736694, Test RMSE 1.023033\n",
      "Training RMSE: 0.732287, Test RMSE 1.023274\n",
      "Training RMSE: 0.721575, Test RMSE 1.023770\n",
      "Training RMSE: 0.701020, Test RMSE 1.024752\n",
      "Training RMSE: 0.676026, Test RMSE 1.026064\n",
      "Training RMSE: 0.651921, Test RMSE 1.027424\n",
      "Training RMSE: 0.629409, Test RMSE 1.028701\n",
      "Training RMSE: 0.609967, Test RMSE 1.029530\n",
      "9724\n",
      "Training RMSE: 0.739412, Test RMSE 1.025442\n",
      "Training RMSE: 0.737661, Test RMSE 1.025535\n",
      "Training RMSE: 0.734157, Test RMSE 1.025835\n",
      "Training RMSE: 0.725103, Test RMSE 1.026539\n",
      "Training RMSE: 0.704364, Test RMSE 1.027873\n",
      "Training RMSE: 0.674592, Test RMSE 1.029547\n",
      "Training RMSE: 0.647463, Test RMSE 1.030829\n",
      "Training RMSE: 0.624912, Test RMSE 1.031700\n",
      "Training RMSE: 0.606889, Test RMSE 1.032341\n",
      "Training RMSE: 0.592753, Test RMSE 1.032637\n",
      "[0.73941182 0.73766124 0.73415695 0.7251026  0.7043636  0.67459206\n",
      " 0.64746277 0.62491181 0.60688903 0.59275322]\n",
      "[1.02544226 1.02553477 1.0258351  1.02653914 1.02787295 1.02954746\n",
      " 1.03082872 1.03169974 1.03234077 1.03263667]\n",
      "num_latent = 10 lambda for user = 0.1 lambda for movie = 0.05\n",
      "9724\n",
      "Training RMSE: 0.735163, Test RMSE 1.050388\n",
      "Training RMSE: 0.733880, Test RMSE 1.050334\n",
      "Training RMSE: 0.731987, Test RMSE 1.050336\n",
      "Training RMSE: 0.728194, Test RMSE 1.050410\n",
      "Training RMSE: 0.719719, Test RMSE 1.050608\n",
      "Training RMSE: 0.702938, Test RMSE 1.051040\n",
      "Training RMSE: 0.679200, Test RMSE 1.051693\n",
      "Training RMSE: 0.654278, Test RMSE 1.052350\n",
      "Training RMSE: 0.631076, Test RMSE 1.052921\n",
      "Training RMSE: 0.610662, Test RMSE 1.053375\n",
      "9724\n",
      "Training RMSE: 0.735017, Test RMSE 1.053453\n",
      "Training RMSE: 0.733809, Test RMSE 1.053306\n",
      "Training RMSE: 0.732067, Test RMSE 1.053092\n",
      "Training RMSE: 0.728571, Test RMSE 1.052780\n",
      "Training RMSE: 0.720524, Test RMSE 1.052376\n",
      "Training RMSE: 0.704485, Test RMSE 1.051995\n",
      "Training RMSE: 0.682534, Test RMSE 1.051825\n",
      "Training RMSE: 0.659911, Test RMSE 1.051936\n",
      "Training RMSE: 0.638087, Test RMSE 1.052228\n",
      "Training RMSE: 0.617860, Test RMSE 1.052654\n",
      "9724\n",
      "Training RMSE: 0.731792, Test RMSE 1.068844\n",
      "Training RMSE: 0.730274, Test RMSE 1.068851\n",
      "Training RMSE: 0.727700, Test RMSE 1.068854\n",
      "Training RMSE: 0.721891, Test RMSE 1.068979\n",
      "Training RMSE: 0.708840, Test RMSE 1.069464\n",
      "Training RMSE: 0.687437, Test RMSE 1.070392\n",
      "Training RMSE: 0.663655, Test RMSE 1.071473\n",
      "Training RMSE: 0.640858, Test RMSE 1.072462\n",
      "Training RMSE: 0.619853, Test RMSE 1.073298\n",
      "Training RMSE: 0.601475, Test RMSE 1.073974\n",
      "9724\n",
      "Training RMSE: 0.739836, Test RMSE 1.022992\n",
      "Training RMSE: 0.738206, Test RMSE 1.022962\n",
      "Training RMSE: 0.735260, Test RMSE 1.023098\n",
      "Training RMSE: 0.728066, Test RMSE 1.023420\n",
      "Training RMSE: 0.711753, Test RMSE 1.024189\n",
      "Training RMSE: 0.686749, Test RMSE 1.025329\n",
      "Training RMSE: 0.661444, Test RMSE 1.026433\n",
      "Training RMSE: 0.638788, Test RMSE 1.027389\n",
      "Training RMSE: 0.619279, Test RMSE 1.028068\n",
      "Training RMSE: 0.602331, Test RMSE 1.028716\n",
      "9724\n",
      "Training RMSE: 0.739365, Test RMSE 1.025155\n",
      "Training RMSE: 0.737645, Test RMSE 1.025061\n",
      "Training RMSE: 0.734164, Test RMSE 1.024985\n",
      "Training RMSE: 0.725026, Test RMSE 1.024945\n",
      "Training RMSE: 0.704602, Test RMSE 1.025213\n",
      "Training RMSE: 0.676610, Test RMSE 1.025808\n",
      "Training RMSE: 0.650471, Test RMSE 1.026469\n",
      "Training RMSE: 0.627890, Test RMSE 1.027183\n",
      "Training RMSE: 0.609415, Test RMSE 1.027661\n",
      "Training RMSE: 0.594478, Test RMSE 1.028072\n",
      "[0.73936513 0.73764521 0.73416351 0.72502563 0.70460249 0.67660987\n",
      " 0.65047093 0.62788967 0.60941513 0.59447754]\n",
      "[1.02515461 1.02506111 1.02498513 1.02494454 1.0252125  1.02580767\n",
      " 1.02646942 1.02718295 1.02766096 1.02807183]\n",
      "num_latent = 10 lambda for user = 0.1 lambda for movie = 0.1\n",
      "9724\n",
      "Training RMSE: 0.735155, Test RMSE 1.050521\n",
      "Training RMSE: 0.733866, Test RMSE 1.050531\n",
      "Training RMSE: 0.731948, Test RMSE 1.050574\n",
      "Training RMSE: 0.728082, Test RMSE 1.050725\n",
      "Training RMSE: 0.719400, Test RMSE 1.051008\n",
      "Training RMSE: 0.702215, Test RMSE 1.051574\n",
      "Training RMSE: 0.678672, Test RMSE 1.052379\n",
      "Training RMSE: 0.654000, Test RMSE 1.053113\n",
      "Training RMSE: 0.631150, Test RMSE 1.053681\n",
      "Training RMSE: 0.610959, Test RMSE 1.054218\n",
      "9724\n",
      "Training RMSE: 0.735070, Test RMSE 1.054013\n",
      "Training RMSE: 0.733785, Test RMSE 1.053962\n",
      "Training RMSE: 0.731973, Test RMSE 1.053933\n",
      "Training RMSE: 0.728307, Test RMSE 1.053984\n",
      "Training RMSE: 0.720104, Test RMSE 1.054095\n",
      "Training RMSE: 0.703799, Test RMSE 1.054520\n",
      "Training RMSE: 0.681002, Test RMSE 1.055271\n",
      "Training RMSE: 0.656052, Test RMSE 1.056266\n",
      "Training RMSE: 0.631940, Test RMSE 1.057240\n",
      "Training RMSE: 0.611801, Test RMSE 1.058017\n",
      "9724\n",
      "Training RMSE: 0.731970, Test RMSE 1.068843\n",
      "Training RMSE: 0.730543, Test RMSE 1.068725\n",
      "Training RMSE: 0.728166, Test RMSE 1.068649\n",
      "Training RMSE: 0.722794, Test RMSE 1.068606\n",
      "Training RMSE: 0.710749, Test RMSE 1.068762\n",
      "Training RMSE: 0.690512, Test RMSE 1.069290\n",
      "Training RMSE: 0.667090, Test RMSE 1.070138\n",
      "Training RMSE: 0.644161, Test RMSE 1.071119\n",
      "Training RMSE: 0.622386, Test RMSE 1.072122\n",
      "Training RMSE: 0.603400, Test RMSE 1.072877\n",
      "9724\n",
      "Training RMSE: 0.739968, Test RMSE 1.022950\n",
      "Training RMSE: 0.738589, Test RMSE 1.022928\n",
      "Training RMSE: 0.736477, Test RMSE 1.022924\n",
      "Training RMSE: 0.731863, Test RMSE 1.022963\n",
      "Training RMSE: 0.720706, Test RMSE 1.023279\n",
      "Training RMSE: 0.699794, Test RMSE 1.023989\n",
      "Training RMSE: 0.674205, Test RMSE 1.024969\n",
      "Training RMSE: 0.649759, Test RMSE 1.025879\n",
      "Training RMSE: 0.628180, Test RMSE 1.026692\n",
      "Training RMSE: 0.610419, Test RMSE 1.027293\n",
      "9724\n",
      "Training RMSE: 0.739525, Test RMSE 1.025147\n",
      "Training RMSE: 0.738103, Test RMSE 1.025138\n",
      "Training RMSE: 0.735670, Test RMSE 1.025188\n",
      "Training RMSE: 0.729724, Test RMSE 1.025299\n",
      "Training RMSE: 0.714646, Test RMSE 1.025749\n",
      "Training RMSE: 0.686814, Test RMSE 1.026630\n",
      "Training RMSE: 0.656490, Test RMSE 1.027599\n",
      "Training RMSE: 0.631196, Test RMSE 1.028425\n",
      "Training RMSE: 0.611558, Test RMSE 1.028959\n",
      "Training RMSE: 0.595921, Test RMSE 1.029419\n",
      "[0.7395246  0.73810267 0.73567046 0.72972362 0.71464571 0.68681362\n",
      " 0.65648987 0.63119617 0.61155773 0.5959208 ]\n",
      "[1.02514747 1.02513808 1.02518841 1.0252987  1.02574935 1.02662962\n",
      " 1.02759874 1.02842549 1.02895904 1.02941915]\n",
      "num_latent = 20 lambda for user = 0.01 lambda for movie = 0.01\n",
      "9724\n",
      "Training RMSE: 0.734348, Test RMSE 1.050906\n",
      "Training RMSE: 0.731693, Test RMSE 1.050762\n",
      "Training RMSE: 0.727539, Test RMSE 1.050680\n",
      "Training RMSE: 0.719061, Test RMSE 1.050712\n",
      "Training RMSE: 0.702514, Test RMSE 1.050881\n",
      "Training RMSE: 0.678631, Test RMSE 1.051276\n",
      "Training RMSE: 0.652269, Test RMSE 1.051844\n",
      "Training RMSE: 0.626736, Test RMSE 1.052456\n",
      "Training RMSE: 0.603768, Test RMSE 1.053045\n",
      "Training RMSE: 0.583679, Test RMSE 1.053551\n",
      "9724\n",
      "Training RMSE: 0.734206, Test RMSE 1.054205\n",
      "Training RMSE: 0.731446, Test RMSE 1.054157\n",
      "Training RMSE: 0.727278, Test RMSE 1.054274\n",
      "Training RMSE: 0.718823, Test RMSE 1.054527\n",
      "Training RMSE: 0.702341, Test RMSE 1.055049\n",
      "Training RMSE: 0.678477, Test RMSE 1.055919\n",
      "Training RMSE: 0.652308, Test RMSE 1.056927\n",
      "Training RMSE: 0.626664, Test RMSE 1.057948\n",
      "Training RMSE: 0.604054, Test RMSE 1.058841\n",
      "Training RMSE: 0.584419, Test RMSE 1.059576\n",
      "9724\n",
      "Training RMSE: 0.731267, Test RMSE 1.069482\n",
      "Training RMSE: 0.728708, Test RMSE 1.069459\n",
      "Training RMSE: 0.725097, Test RMSE 1.069529\n",
      "Training RMSE: 0.717971, Test RMSE 1.069841\n",
      "Training RMSE: 0.702952, Test RMSE 1.070454\n",
      "Training RMSE: 0.678835, Test RMSE 1.071499\n",
      "Training RMSE: 0.651519, Test RMSE 1.072731\n",
      "Training RMSE: 0.625397, Test RMSE 1.073904\n",
      "Training RMSE: 0.601872, Test RMSE 1.074918\n",
      "Training RMSE: 0.581662, Test RMSE 1.075747\n",
      "9724\n",
      "Training RMSE: 0.739133, Test RMSE 1.023111\n",
      "Training RMSE: 0.736220, Test RMSE 1.023106\n",
      "Training RMSE: 0.731504, Test RMSE 1.023233\n",
      "Training RMSE: 0.721127, Test RMSE 1.023575\n",
      "Training RMSE: 0.700104, Test RMSE 1.024445\n",
      "Training RMSE: 0.671865, Test RMSE 1.025576\n",
      "Training RMSE: 0.644076, Test RMSE 1.026658\n",
      "Training RMSE: 0.619207, Test RMSE 1.027464\n",
      "Training RMSE: 0.597875, Test RMSE 1.028155\n",
      "Training RMSE: 0.579253, Test RMSE 1.028787\n",
      "9724\n",
      "Training RMSE: 0.738691, Test RMSE 1.025318\n",
      "Training RMSE: 0.735453, Test RMSE 1.025207\n",
      "Training RMSE: 0.729385, Test RMSE 1.025195\n",
      "Training RMSE: 0.714963, Test RMSE 1.025412\n",
      "Training RMSE: 0.688641, Test RMSE 1.026020\n",
      "Training RMSE: 0.658958, Test RMSE 1.026865\n",
      "Training RMSE: 0.632759, Test RMSE 1.027612\n",
      "Training RMSE: 0.610481, Test RMSE 1.028333\n",
      "Training RMSE: 0.591930, Test RMSE 1.028893\n",
      "Training RMSE: 0.576507, Test RMSE 1.029240\n",
      "[0.7386906  0.73545307 0.72938518 0.71496335 0.68864136 0.65895836\n",
      " 0.63275871 0.61048103 0.59193003 0.57650653]\n",
      "[1.02531808 1.02520669 1.02519458 1.02541175 1.02601977 1.02686541\n",
      " 1.0276124  1.02833341 1.02889265 1.02923988]\n",
      "num_latent = 20 lambda for user = 0.01 lambda for movie = 0.05\n",
      "9724\n",
      "Training RMSE: 0.734345, Test RMSE 1.050519\n",
      "Training RMSE: 0.731819, Test RMSE 1.050467\n",
      "Training RMSE: 0.728155, Test RMSE 1.050609\n",
      "Training RMSE: 0.721006, Test RMSE 1.050856\n",
      "Training RMSE: 0.706704, Test RMSE 1.051394\n",
      "Training RMSE: 0.683850, Test RMSE 1.052187\n",
      "Training RMSE: 0.657426, Test RMSE 1.052984\n",
      "Training RMSE: 0.631460, Test RMSE 1.053619\n",
      "Training RMSE: 0.607406, Test RMSE 1.054188\n",
      "Training RMSE: 0.586432, Test RMSE 1.054616\n",
      "9724\n",
      "Training RMSE: 0.734240, Test RMSE 1.054998\n",
      "Training RMSE: 0.731432, Test RMSE 1.054929\n",
      "Training RMSE: 0.726887, Test RMSE 1.055232\n",
      "Training RMSE: 0.717562, Test RMSE 1.055970\n",
      "Training RMSE: 0.699936, Test RMSE 1.057294\n",
      "Training RMSE: 0.676669, Test RMSE 1.058945\n",
      "Training RMSE: 0.652591, Test RMSE 1.060490\n",
      "Training RMSE: 0.628459, Test RMSE 1.061939\n",
      "Training RMSE: 0.606104, Test RMSE 1.063145\n",
      "Training RMSE: 0.586545, Test RMSE 1.064029\n",
      "9724\n",
      "Training RMSE: 0.731248, Test RMSE 1.069090\n",
      "Training RMSE: 0.728567, Test RMSE 1.069120\n",
      "Training RMSE: 0.724611, Test RMSE 1.069252\n",
      "Training RMSE: 0.716631, Test RMSE 1.069676\n",
      "Training RMSE: 0.699856, Test RMSE 1.070500\n",
      "Training RMSE: 0.673071, Test RMSE 1.071838\n",
      "Training RMSE: 0.644096, Test RMSE 1.073154\n",
      "Training RMSE: 0.618141, Test RMSE 1.074204\n",
      "Training RMSE: 0.595877, Test RMSE 1.075012\n",
      "Training RMSE: 0.577244, Test RMSE 1.075647\n",
      "9724\n",
      "Training RMSE: 0.739090, Test RMSE 1.023033\n",
      "Training RMSE: 0.736211, Test RMSE 1.022984\n",
      "Training RMSE: 0.731478, Test RMSE 1.023052\n",
      "Training RMSE: 0.721177, Test RMSE 1.023349\n",
      "Training RMSE: 0.700888, Test RMSE 1.024162\n",
      "Training RMSE: 0.674156, Test RMSE 1.025349\n",
      "Training RMSE: 0.647577, Test RMSE 1.026598\n",
      "Training RMSE: 0.623103, Test RMSE 1.027627\n",
      "Training RMSE: 0.601699, Test RMSE 1.028451\n",
      "Training RMSE: 0.582797, Test RMSE 1.029120\n",
      "9724\n",
      "Training RMSE: 0.738769, Test RMSE 1.026072\n",
      "Training RMSE: 0.736116, Test RMSE 1.026104\n",
      "Training RMSE: 0.732012, Test RMSE 1.026412\n",
      "Training RMSE: 0.722960, Test RMSE 1.027071\n",
      "Training RMSE: 0.702977, Test RMSE 1.028433\n",
      "Training RMSE: 0.672444, Test RMSE 1.030267\n",
      "Training RMSE: 0.642133, Test RMSE 1.031942\n",
      "Training RMSE: 0.615827, Test RMSE 1.033137\n",
      "Training RMSE: 0.594495, Test RMSE 1.033967\n",
      "Training RMSE: 0.576699, Test RMSE 1.034654\n",
      "[0.73876927 0.73611587 0.7320121  0.72295962 0.70297724 0.67244425\n",
      " 0.64213328 0.61582723 0.59449461 0.57669883]\n",
      "[1.02607244 1.02610378 1.02641248 1.02707054 1.02843348 1.03026733\n",
      " 1.03194202 1.03313719 1.03396692 1.03465386]\n",
      "num_latent = 20 lambda for user = 0.01 lambda for movie = 0.1\n",
      "9724\n",
      "Training RMSE: 0.734421, Test RMSE 1.050810\n",
      "Training RMSE: 0.731609, Test RMSE 1.050748\n",
      "Training RMSE: 0.727301, Test RMSE 1.050775\n",
      "Training RMSE: 0.718623, Test RMSE 1.050896\n",
      "Training RMSE: 0.701506, Test RMSE 1.051187\n",
      "Training RMSE: 0.675858, Test RMSE 1.051778\n",
      "Training RMSE: 0.647416, Test RMSE 1.052451\n",
      "Training RMSE: 0.621335, Test RMSE 1.053047\n",
      "Training RMSE: 0.598988, Test RMSE 1.053498\n",
      "Training RMSE: 0.580512, Test RMSE 1.053826\n",
      "9724\n",
      "Training RMSE: 0.734016, Test RMSE 1.054152\n",
      "Training RMSE: 0.731118, Test RMSE 1.054081\n",
      "Training RMSE: 0.726377, Test RMSE 1.054044\n",
      "Training RMSE: 0.716503, Test RMSE 1.054053\n",
      "Training RMSE: 0.697996, Test RMSE 1.054293\n",
      "Training RMSE: 0.673205, Test RMSE 1.054853\n",
      "Training RMSE: 0.647525, Test RMSE 1.055623\n",
      "Training RMSE: 0.623128, Test RMSE 1.056471\n",
      "Training RMSE: 0.601577, Test RMSE 1.057157\n",
      "Training RMSE: 0.582929, Test RMSE 1.057810\n",
      "9724\n",
      "Training RMSE: 0.731232, Test RMSE 1.069231\n",
      "Training RMSE: 0.728508, Test RMSE 1.069227\n",
      "Training RMSE: 0.724382, Test RMSE 1.069445\n",
      "Training RMSE: 0.715882, Test RMSE 1.069944\n",
      "Training RMSE: 0.698346, Test RMSE 1.070978\n",
      "Training RMSE: 0.672241, Test RMSE 1.072452\n",
      "Training RMSE: 0.644411, Test RMSE 1.073897\n",
      "Training RMSE: 0.618650, Test RMSE 1.075061\n",
      "Training RMSE: 0.596582, Test RMSE 1.075855\n",
      "Training RMSE: 0.577708, Test RMSE 1.076492\n",
      "9724\n",
      "Training RMSE: 0.739221, Test RMSE 1.023576\n",
      "Training RMSE: 0.736599, Test RMSE 1.023523\n",
      "Training RMSE: 0.732599, Test RMSE 1.023666\n",
      "Training RMSE: 0.724054, Test RMSE 1.024041\n",
      "Training RMSE: 0.705949, Test RMSE 1.024865\n",
      "Training RMSE: 0.679417, Test RMSE 1.026086\n",
      "Training RMSE: 0.652306, Test RMSE 1.027243\n",
      "Training RMSE: 0.627560, Test RMSE 1.028137\n",
      "Training RMSE: 0.605904, Test RMSE 1.028879\n",
      "Training RMSE: 0.586436, Test RMSE 1.029545\n",
      "9724\n",
      "Training RMSE: 0.738658, Test RMSE 1.025706\n",
      "Training RMSE: 0.735702, Test RMSE 1.025912\n",
      "Training RMSE: 0.730406, Test RMSE 1.026365\n",
      "Training RMSE: 0.717873, Test RMSE 1.027276\n",
      "Training RMSE: 0.692503, Test RMSE 1.028926\n",
      "Training RMSE: 0.661309, Test RMSE 1.030586\n",
      "Training RMSE: 0.633166, Test RMSE 1.031897\n",
      "Training RMSE: 0.609494, Test RMSE 1.032885\n",
      "Training RMSE: 0.590484, Test RMSE 1.033405\n",
      "Training RMSE: 0.574285, Test RMSE 1.034025\n",
      "[0.73865773 0.7357017  0.73040555 0.71787267 0.69250308 0.66130933\n",
      " 0.63316582 0.60949431 0.59048394 0.57428518]\n",
      "[1.02570639 1.02591171 1.02636524 1.02727599 1.0289262  1.03058565\n",
      " 1.03189719 1.03288528 1.03340542 1.0340254 ]\n",
      "num_latent = 20 lambda for user = 0.05 lambda for movie = 0.01\n",
      "9724\n",
      "Training RMSE: 0.734239, Test RMSE 1.050618\n",
      "Training RMSE: 0.731585, Test RMSE 1.050533\n",
      "Training RMSE: 0.727661, Test RMSE 1.050530\n",
      "Training RMSE: 0.719983, Test RMSE 1.050606\n",
      "Training RMSE: 0.704963, Test RMSE 1.050798\n",
      "Training RMSE: 0.681638, Test RMSE 1.051233\n",
      "Training RMSE: 0.654946, Test RMSE 1.051818\n",
      "Training RMSE: 0.628406, Test RMSE 1.052479\n",
      "Training RMSE: 0.604273, Test RMSE 1.053104\n",
      "Training RMSE: 0.583726, Test RMSE 1.053661\n",
      "9724\n",
      "Training RMSE: 0.734111, Test RMSE 1.053880\n",
      "Training RMSE: 0.731383, Test RMSE 1.053881\n",
      "Training RMSE: 0.727290, Test RMSE 1.054053\n",
      "Training RMSE: 0.718962, Test RMSE 1.054399\n",
      "Training RMSE: 0.702337, Test RMSE 1.055075\n",
      "Training RMSE: 0.677487, Test RMSE 1.056186\n",
      "Training RMSE: 0.651059, Test RMSE 1.057351\n",
      "Training RMSE: 0.626554, Test RMSE 1.058396\n",
      "Training RMSE: 0.604537, Test RMSE 1.059227\n",
      "Training RMSE: 0.585376, Test RMSE 1.059976\n",
      "9724\n",
      "Training RMSE: 0.731330, Test RMSE 1.069353\n",
      "Training RMSE: 0.728797, Test RMSE 1.069370\n",
      "Training RMSE: 0.725233, Test RMSE 1.069425\n",
      "Training RMSE: 0.718256, Test RMSE 1.069599\n",
      "Training RMSE: 0.703594, Test RMSE 1.069999\n",
      "Training RMSE: 0.679181, Test RMSE 1.070892\n",
      "Training RMSE: 0.650735, Test RMSE 1.072134\n",
      "Training RMSE: 0.623740, Test RMSE 1.073409\n",
      "Training RMSE: 0.600667, Test RMSE 1.074441\n",
      "Training RMSE: 0.581040, Test RMSE 1.075270\n",
      "9724\n",
      "Training RMSE: 0.739254, Test RMSE 1.023099\n",
      "Training RMSE: 0.736201, Test RMSE 1.022919\n",
      "Training RMSE: 0.731002, Test RMSE 1.022831\n",
      "Training RMSE: 0.719392, Test RMSE 1.022931\n",
      "Training RMSE: 0.696512, Test RMSE 1.023376\n",
      "Training RMSE: 0.667169, Test RMSE 1.024181\n",
      "Training RMSE: 0.639634, Test RMSE 1.025101\n",
      "Training RMSE: 0.616060, Test RMSE 1.025764\n",
      "Training RMSE: 0.596174, Test RMSE 1.026287\n",
      "Training RMSE: 0.579350, Test RMSE 1.026655\n",
      "9724\n",
      "Training RMSE: 0.738911, Test RMSE 1.025523\n",
      "Training RMSE: 0.736450, Test RMSE 1.025404\n",
      "Training RMSE: 0.732897, Test RMSE 1.025480\n",
      "Training RMSE: 0.725515, Test RMSE 1.025808\n",
      "Training RMSE: 0.708397, Test RMSE 1.026657\n",
      "Training RMSE: 0.678105, Test RMSE 1.028185\n",
      "Training RMSE: 0.645147, Test RMSE 1.029860\n",
      "Training RMSE: 0.618331, Test RMSE 1.031139\n",
      "Training RMSE: 0.597538, Test RMSE 1.031958\n",
      "Training RMSE: 0.580169, Test RMSE 1.032681\n",
      "[0.73891087 0.73645016 0.7328969  0.72551508 0.70839735 0.67810466\n",
      " 0.6451469  0.61833118 0.59753756 0.58016859]\n",
      "[1.02552313 1.02540445 1.0254796  1.02580812 1.02665671 1.02818509\n",
      " 1.0298595  1.03113855 1.03195756 1.03268099]\n",
      "num_latent = 20 lambda for user = 0.05 lambda for movie = 0.05\n",
      "9724\n",
      "Training RMSE: 0.734544, Test RMSE 1.050570\n",
      "Training RMSE: 0.732177, Test RMSE 1.050502\n",
      "Training RMSE: 0.728943, Test RMSE 1.050519\n",
      "Training RMSE: 0.722885, Test RMSE 1.050586\n",
      "Training RMSE: 0.710273, Test RMSE 1.050788\n",
      "Training RMSE: 0.688679, Test RMSE 1.051243\n",
      "Training RMSE: 0.661795, Test RMSE 1.051902\n",
      "Training RMSE: 0.635254, Test RMSE 1.052556\n",
      "Training RMSE: 0.610246, Test RMSE 1.053176\n",
      "Training RMSE: 0.589179, Test RMSE 1.053694\n",
      "9724\n",
      "Training RMSE: 0.734301, Test RMSE 1.054094\n",
      "Training RMSE: 0.731829, Test RMSE 1.054017\n",
      "Training RMSE: 0.728467, Test RMSE 1.053928\n",
      "Training RMSE: 0.722126, Test RMSE 1.053919\n",
      "Training RMSE: 0.709223, Test RMSE 1.054077\n",
      "Training RMSE: 0.686268, Test RMSE 1.054444\n",
      "Training RMSE: 0.657066, Test RMSE 1.055151\n",
      "Training RMSE: 0.629201, Test RMSE 1.055904\n",
      "Training RMSE: 0.605255, Test RMSE 1.056555\n",
      "Training RMSE: 0.585225, Test RMSE 1.057162\n",
      "9724\n",
      "Training RMSE: 0.731299, Test RMSE 1.069481\n",
      "Training RMSE: 0.728881, Test RMSE 1.069351\n",
      "Training RMSE: 0.725577, Test RMSE 1.069365\n",
      "Training RMSE: 0.719290, Test RMSE 1.069486\n",
      "Training RMSE: 0.705674, Test RMSE 1.069838\n",
      "Training RMSE: 0.681798, Test RMSE 1.070614\n",
      "Training RMSE: 0.653051, Test RMSE 1.071829\n",
      "Training RMSE: 0.626475, Test RMSE 1.072963\n",
      "Training RMSE: 0.603122, Test RMSE 1.074024\n",
      "Training RMSE: 0.583166, Test RMSE 1.074823\n",
      "9724\n",
      "Training RMSE: 0.739269, Test RMSE 1.023383\n",
      "Training RMSE: 0.736626, Test RMSE 1.023336\n",
      "Training RMSE: 0.732710, Test RMSE 1.023385\n",
      "Training RMSE: 0.724598, Test RMSE 1.023549\n",
      "Training RMSE: 0.706867, Test RMSE 1.023975\n",
      "Training RMSE: 0.678582, Test RMSE 1.024863\n",
      "Training RMSE: 0.649262, Test RMSE 1.025906\n",
      "Training RMSE: 0.624046, Test RMSE 1.026745\n",
      "Training RMSE: 0.602188, Test RMSE 1.027484\n",
      "Training RMSE: 0.583186, Test RMSE 1.028206\n",
      "9724\n",
      "Training RMSE: 0.738651, Test RMSE 1.025434\n",
      "Training RMSE: 0.735680, Test RMSE 1.025468\n",
      "Training RMSE: 0.730449, Test RMSE 1.025712\n",
      "Training RMSE: 0.718169, Test RMSE 1.026333\n",
      "Training RMSE: 0.693820, Test RMSE 1.027609\n",
      "Training RMSE: 0.663332, Test RMSE 1.029099\n",
      "Training RMSE: 0.635363, Test RMSE 1.030329\n",
      "Training RMSE: 0.611636, Test RMSE 1.031351\n",
      "Training RMSE: 0.592234, Test RMSE 1.032084\n",
      "Training RMSE: 0.575670, Test RMSE 1.032710\n",
      "[0.7386512  0.73568009 0.73044867 0.71816907 0.69381975 0.66333168\n",
      " 0.63536296 0.61163574 0.59223371 0.57566989]\n",
      "[1.02543397 1.02546814 1.02571204 1.02633318 1.02760859 1.02909856\n",
      " 1.03032945 1.03135066 1.03208377 1.03270952]\n",
      "num_latent = 20 lambda for user = 0.05 lambda for movie = 0.1\n",
      "9724\n",
      "Training RMSE: 0.734123, Test RMSE 1.050755\n",
      "Training RMSE: 0.731341, Test RMSE 1.050660\n",
      "Training RMSE: 0.727106, Test RMSE 1.050635\n",
      "Training RMSE: 0.718694, Test RMSE 1.050652\n",
      "Training RMSE: 0.702043, Test RMSE 1.050815\n",
      "Training RMSE: 0.676695, Test RMSE 1.051170\n",
      "Training RMSE: 0.648267, Test RMSE 1.051676\n",
      "Training RMSE: 0.621658, Test RMSE 1.052143\n",
      "Training RMSE: 0.598699, Test RMSE 1.052592\n",
      "Training RMSE: 0.579008, Test RMSE 1.052928\n",
      "9724\n",
      "Training RMSE: 0.734172, Test RMSE 1.054358\n",
      "Training RMSE: 0.731467, Test RMSE 1.054424\n",
      "Training RMSE: 0.727629, Test RMSE 1.054763\n",
      "Training RMSE: 0.720254, Test RMSE 1.055425\n",
      "Training RMSE: 0.705361, Test RMSE 1.056650\n",
      "Training RMSE: 0.680920, Test RMSE 1.058497\n",
      "Training RMSE: 0.652505, Test RMSE 1.060481\n",
      "Training RMSE: 0.626127, Test RMSE 1.062138\n",
      "Training RMSE: 0.603428, Test RMSE 1.063420\n",
      "Training RMSE: 0.584370, Test RMSE 1.064334\n",
      "9724\n",
      "Training RMSE: 0.731362, Test RMSE 1.069318\n",
      "Training RMSE: 0.728841, Test RMSE 1.069407\n",
      "Training RMSE: 0.725370, Test RMSE 1.069746\n",
      "Training RMSE: 0.718806, Test RMSE 1.070420\n",
      "Training RMSE: 0.704909, Test RMSE 1.071710\n",
      "Training RMSE: 0.680435, Test RMSE 1.073686\n",
      "Training RMSE: 0.650656, Test RMSE 1.075869\n",
      "Training RMSE: 0.623389, Test RMSE 1.077531\n",
      "Training RMSE: 0.600140, Test RMSE 1.078679\n",
      "Training RMSE: 0.580828, Test RMSE 1.079451\n",
      "9724\n",
      "Training RMSE: 0.739074, Test RMSE 1.023217\n",
      "Training RMSE: 0.736159, Test RMSE 1.023269\n",
      "Training RMSE: 0.731365, Test RMSE 1.023513\n",
      "Training RMSE: 0.720903, Test RMSE 1.024081\n",
      "Training RMSE: 0.699544, Test RMSE 1.025178\n",
      "Training RMSE: 0.670280, Test RMSE 1.026637\n",
      "Training RMSE: 0.641480, Test RMSE 1.028028\n",
      "Training RMSE: 0.616492, Test RMSE 1.029120\n",
      "Training RMSE: 0.595721, Test RMSE 1.029895\n",
      "Training RMSE: 0.578249, Test RMSE 1.030457\n",
      "9724\n",
      "Training RMSE: 0.738584, Test RMSE 1.025466\n",
      "Training RMSE: 0.735885, Test RMSE 1.025388\n",
      "Training RMSE: 0.731456, Test RMSE 1.025440\n",
      "Training RMSE: 0.721346, Test RMSE 1.025725\n",
      "Training RMSE: 0.699951, Test RMSE 1.026469\n",
      "Training RMSE: 0.670781, Test RMSE 1.027427\n",
      "Training RMSE: 0.642355, Test RMSE 1.028336\n",
      "Training RMSE: 0.617135, Test RMSE 1.029148\n",
      "Training RMSE: 0.596245, Test RMSE 1.029811\n",
      "Training RMSE: 0.578779, Test RMSE 1.030328\n",
      "[0.73858366 0.73588495 0.73145615 0.72134581 0.69995069 0.67078092\n",
      " 0.64235514 0.61713503 0.59624498 0.57877926]\n",
      "[1.0254656  1.02538842 1.02543989 1.02572484 1.02646887 1.02742665\n",
      " 1.02833641 1.02914775 1.02981129 1.03032774]\n",
      "num_latent = 20 lambda for user = 0.1 lambda for movie = 0.01\n",
      "9724\n",
      "Training RMSE: 0.734551, Test RMSE 1.050879\n",
      "Training RMSE: 0.732083, Test RMSE 1.051003\n",
      "Training RMSE: 0.728469, Test RMSE 1.051284\n",
      "Training RMSE: 0.721398, Test RMSE 1.051857\n",
      "Training RMSE: 0.706913, Test RMSE 1.052768\n",
      "Training RMSE: 0.683363, Test RMSE 1.054069\n",
      "Training RMSE: 0.656033, Test RMSE 1.055415\n",
      "Training RMSE: 0.629695, Test RMSE 1.056470\n",
      "Training RMSE: 0.606554, Test RMSE 1.057244\n",
      "Training RMSE: 0.586613, Test RMSE 1.057831\n",
      "9724\n",
      "Training RMSE: 0.734258, Test RMSE 1.054318\n",
      "Training RMSE: 0.731395, Test RMSE 1.054246\n",
      "Training RMSE: 0.726888, Test RMSE 1.054322\n",
      "Training RMSE: 0.717563, Test RMSE 1.054676\n",
      "Training RMSE: 0.699077, Test RMSE 1.055436\n",
      "Training RMSE: 0.672590, Test RMSE 1.056550\n",
      "Training RMSE: 0.645048, Test RMSE 1.057707\n",
      "Training RMSE: 0.620274, Test RMSE 1.058710\n",
      "Training RMSE: 0.599637, Test RMSE 1.059564\n",
      "Training RMSE: 0.581969, Test RMSE 1.060324\n",
      "9724\n",
      "Training RMSE: 0.731219, Test RMSE 1.069023\n",
      "Training RMSE: 0.728635, Test RMSE 1.068988\n",
      "Training RMSE: 0.724861, Test RMSE 1.069069\n",
      "Training RMSE: 0.717290, Test RMSE 1.069308\n",
      "Training RMSE: 0.701796, Test RMSE 1.069911\n",
      "Training RMSE: 0.677883, Test RMSE 1.071048\n",
      "Training RMSE: 0.651795, Test RMSE 1.072325\n",
      "Training RMSE: 0.626236, Test RMSE 1.073634\n",
      "Training RMSE: 0.603112, Test RMSE 1.074783\n",
      "Training RMSE: 0.582990, Test RMSE 1.075704\n",
      "9724\n",
      "Training RMSE: 0.739047, Test RMSE 1.023501\n",
      "Training RMSE: 0.736020, Test RMSE 1.023606\n",
      "Training RMSE: 0.730940, Test RMSE 1.023967\n",
      "Training RMSE: 0.719713, Test RMSE 1.024711\n",
      "Training RMSE: 0.698012, Test RMSE 1.026005\n",
      "Training RMSE: 0.670247, Test RMSE 1.027606\n",
      "Training RMSE: 0.643508, Test RMSE 1.029023\n",
      "Training RMSE: 0.619535, Test RMSE 1.030256\n",
      "Training RMSE: 0.598895, Test RMSE 1.031283\n",
      "Training RMSE: 0.581061, Test RMSE 1.032041\n",
      "9724\n",
      "Training RMSE: 0.738865, Test RMSE 1.025657\n",
      "Training RMSE: 0.736193, Test RMSE 1.025482\n",
      "Training RMSE: 0.731988, Test RMSE 1.025407\n",
      "Training RMSE: 0.722438, Test RMSE 1.025515\n",
      "Training RMSE: 0.701160, Test RMSE 1.026016\n",
      "Training RMSE: 0.670301, Test RMSE 1.026833\n",
      "Training RMSE: 0.640728, Test RMSE 1.027637\n",
      "Training RMSE: 0.615801, Test RMSE 1.028239\n",
      "Training RMSE: 0.595333, Test RMSE 1.028712\n",
      "Training RMSE: 0.578189, Test RMSE 1.029062\n",
      "[0.73886509 0.73619265 0.73198817 0.72243826 0.70115979 0.67030144\n",
      " 0.64072822 0.61580072 0.59533299 0.57818893]\n",
      "[1.02565716 1.02548153 1.02540685 1.0255149  1.02601618 1.02683303\n",
      " 1.02763686 1.02823889 1.02871225 1.02906225]\n",
      "num_latent = 20 lambda for user = 0.1 lambda for movie = 0.05\n",
      "9724\n",
      "Training RMSE: 0.734304, Test RMSE 1.050740\n",
      "Training RMSE: 0.731701, Test RMSE 1.050710\n",
      "Training RMSE: 0.728025, Test RMSE 1.050764\n",
      "Training RMSE: 0.720967, Test RMSE 1.050974\n",
      "Training RMSE: 0.706823, Test RMSE 1.051318\n",
      "Training RMSE: 0.683468, Test RMSE 1.051991\n",
      "Training RMSE: 0.655599, Test RMSE 1.052839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-adf3b893810e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtest_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-167-476a976fc3b4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_vec, test_vec)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     pred_out = np.sum(np.multiply(self.w_User[np.array(train_vec[:, 0], dtype='int32'), :],\n\u001b[0;32m---> 93\u001b[0;31m                                                   self.w_Item[np.array(train_vec[:, 1], dtype='int32'), :]),\n\u001b[0m\u001b[1;32m     94\u001b[0m                                       axis=1)  # mean_inv subtracted\n\u001b[1;32m     95\u001b[0m                     \u001b[0mrawErr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_out\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_inv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,8)\n",
    "\n",
    "for i in num_feat_list:\n",
    "    for j in lambda1_list:\n",
    "        for z in lambda2_list:\n",
    "            pmf.set_params({\"num_feat\": i, \"epsilon\": 1, \"_lambda\": j, \"momentum\": 0.8, \"maxepoch\": 10, \"num_batches\": 100,\n",
    "                    \"batch_size\": 1000})\n",
    "            print('num_latent =',i,'lambda for user =',j,'lambda for movie =',z)\n",
    "            kf = KFold(n_splits=5)\n",
    "            train_error=[]\n",
    "            test_error=[]\n",
    "            for train_index, test_index in kf.split(final_data):\n",
    "                X_train, X_test = final_data[train_index], final_data[test_index]\n",
    "                pmf.fit(X_train, X_test)\n",
    "            train_error.append(pmf.rmse_train[-10:])\n",
    "            test_error.append(pmf.rmse_test[-10:])\n",
    "            RMSE_train=np.mean(train_error,axis=0)\n",
    "            RMSE_test=np.mean(test_error,axis=0)\n",
    "            print('For Training DataSet',RMSE_train)\n",
    "            print('For Testing DataSet',RMSE_test)\n",
    "            axs[i].plot()\n",
    "plt.plot(range(pmf.maxepoch), pmf.rmse_train, marker='o', label='Training Data')\n",
    "plt.plot(range(pmf.maxepoch), pmf.rmse_test, marker='v', label='Test Data')\n",
    "plt.title('The MovieRating Dataset Learning Curve')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.43424716, 3.37180204, 3.3881135 , ..., 3.44704004, 3.53734453,\n",
       "        3.46787001],\n",
       "       [4.30212109, 3.76408681, 3.66534113, ..., 3.57861016, 3.49455719,\n",
       "        3.53965215],\n",
       "       [3.53063663, 3.57472247, 3.55362408, ..., 3.50121836, 3.48090632,\n",
       "        3.50198032],\n",
       "       ...,\n",
       "       [3.62664609, 3.63409309, 3.65161604, ..., 3.41078218, 3.53346575,\n",
       "        3.42999262],\n",
       "       [3.61838983, 3.00923286, 2.90620857, ..., 3.62595927, 3.50642925,\n",
       "        3.46644537],\n",
       "       [3.45118029, 3.36972855, 3.44191247, ..., 3.4783898 , 3.49634485,\n",
       "        3.50504771]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_matrix_final=a[:-1,np.unique(data[:, 1]).astype(int)]\n",
    "pred_matrix_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pred_matrix_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.486883246437346"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(pred_matrix_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.083585193767703"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(pred_matrix_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000e+00, 1.00000e+00, 1.00000e+00, ..., 1.93585e+05,\n",
       "       1.93587e+05, 1.93609e+05])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.argsort(data[:,1])\n",
    "data[:, 1][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data[data[:,1].argsort()]\n",
    "len(np.unique(new_data[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(new_data[:, 1])\n",
    "le.classes_\n",
    "a=le.transform(new_data[:, 1])\n",
    "c=np.array(a).reshape(100836,1)\n",
    "np.shape(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 3)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 275., 1333.,    4., 1021.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = np.concatenate((new_data,c),axis=1)\n",
    "final_data[30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=pd.DataFrame(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.columns = ['user','moive','rating',\n",
    "                     'new_movie_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff1=array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
